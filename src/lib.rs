use crate::anvil_api::auth_service_server::AuthServiceServer;
use crate::anvil_api::bucket_service_server::BucketServiceServer;
use crate::anvil_api::internal_anvil_service_server::InternalAnvilServiceServer;
use crate::anvil_api::object_service_server::ObjectServiceServer;
use crate::auth::JwtManager;
use crate::config::Config;
use anyhow::Result;
use cluster::ClusterState;
use deadpool_postgres::{ManagerConfig, Pool, RecyclingMethod};
use std::collections::HashMap;
use std::str::FromStr;
use std::sync::Arc;
use tokio::sync::RwLock;
use tokio_postgres::NoTls;

// The modules we've created
pub mod auth;
pub mod bucket_manager;
pub mod cluster;
pub mod config;
pub mod crypto;
pub mod discovery;
pub mod middleware;
pub mod object_manager;
pub mod persistence;
pub mod placement;
pub mod s3_auth;
pub mod s3_gateway;
pub mod services;
pub mod sharding;
pub mod storage;
pub mod tasks;
pub mod validation;
pub mod worker;

// The gRPC code generated by tonic-build
pub mod anvil_api {
    tonic::include_proto!("anvil");
}

pub mod migrations {
    use refinery_macros::embed_migrations;
    embed_migrations!("./migrations_global");
}

pub mod regional_migrations {
    use refinery_macros::embed_migrations;
    embed_migrations!("./migrations_regional");
}

// Our application state, which will hold the persistence layer, storage engine, etc.
#[derive(Clone)]
pub struct AppState {
    pub db: persistence::Persistence,
    pub storage: storage::Storage,
    pub cluster: ClusterState,
    pub sharder: sharding::ShardManager,
    pub placer: placement::PlacementManager,
    pub jwt_manager: Arc<JwtManager>,
    pub region: String,
    pub bucket_manager: bucket_manager::BucketManager,
    pub object_manager: object_manager::ObjectManager,
    pub config: Arc<Config>,
}

impl AppState {
    pub async fn new(global_pool: Pool, regional_pool: Pool, config: Config) -> Result<Self> {
        let arc_config = Arc::new(config);
        let jwt_manager = Arc::new(JwtManager::new(arc_config.jwt_secret.clone()));
        let storage = storage::Storage::new().await?;
        let cluster_state = Arc::new(RwLock::new(HashMap::new()));
        let db = persistence::Persistence::new(global_pool, regional_pool);
        let sharder = sharding::ShardManager::new();
        let placer = placement::PlacementManager::default();

        let bucket_manager = bucket_manager::BucketManager::new(db.clone());
        let object_manager = object_manager::ObjectManager::new(
            db.clone(),
            placer.clone(),
            cluster_state.clone(),
            sharder.clone(),
            storage.clone(),
            arc_config.region.clone(),
            jwt_manager.clone(),
            arc_config.worka_secret_encryption_key.clone(),
        );

        Ok(Self {
            db,
            storage,
            cluster: cluster_state,
            sharder,
            placer,
            jwt_manager,
            region: arc_config.region.clone(),
            bucket_manager,
            object_manager,
            config: arc_config,
        })
    }
}

pub async fn run(listener: tokio::net::TcpListener, config: Config) -> Result<()> {
    // Run migrations first
    run_migrations(
        &config.global_database_url,
        migrations::migrations::runner(),
        "refinery_schema_history_global",
    )
    .await?;
    run_migrations(
        &config.regional_database_url,
        regional_migrations::migrations::runner(),
        "refinery_schema_history_regional",
    )
    .await?;

    let regional_pool = create_pool(&config.regional_database_url)?;
    let global_pool = create_pool(&config.global_database_url)?;
    let state = AppState::new(global_pool, regional_pool, config).await?;
    let swarm = cluster::create_swarm(state.config.clone()).await?;

    // Then start the node
    start_node(listener, state, swarm).await
}

pub async fn start_node(
    listener: tokio::net::TcpListener,
    state: AppState,
    mut swarm: libp2p::Swarm<cluster::ClusterBehaviour>,
) -> Result<()> {
    for addr in &state.config.bootstrap_addrs {
        let multiaddr: libp2p::Multiaddr = addr.parse()?;
        swarm.dial(multiaddr)?;
    }

    let worker_state = state.clone();
    tokio::spawn(async move {
        if let Err(e) = worker::run(
            worker_state.db.clone(),
            worker_state.cluster.clone(),
            worker_state.jwt_manager.clone(),
        )
        .await
        {
            eprintln!("Worker process failed: {}", e);
        }
    });

    // --- Services ---
    let state_clone = state.clone();
    let auth_interceptor = move |req| middleware::auth_interceptor(req, &state_clone);

    // Create the gRPC router, applying the interceptor to each protected service.
    let grpc_router = tonic::service::Routes::new(AuthServiceServer::with_interceptor(
        state.clone(),
        auth_interceptor.clone(),
    ))
    .add_service(ObjectServiceServer::with_interceptor(
        state.clone(),
        auth_interceptor.clone(),
    ))
    .add_service(BucketServiceServer::with_interceptor(
        state.clone(),
        auth_interceptor.clone(),
    ))
    .add_service(InternalAnvilServiceServer::with_interceptor(
        state.clone(),
        auth_interceptor,
    ));

    // Create the Axum router for S3 and mount the gRPC router under /grpc to avoid route conflicts.
    let grpc_routes = grpc_router
        .into_axum_router()
        // layer runs on the HTTP request *before* tonicâ€™s gRPC handling
        // This is necessary for the interceptor to get access to the URI because newer tonic versions do not provide a way to get it in the interceptor
        // later after the interceptor it is available via req.extensions().get::<tonic::GrpcMethod>()
        .route_layer(axum::middleware::from_fn(middleware::save_uri_mw));

    let app = s3_gateway::app(state.clone())
        .nest("/grpc", grpc_routes);

    let addr = listener.local_addr()?;
    println!("Anvil server (gRPC & S3) listening on {}", addr);

    // Spawn the gossip service to run in the background.
    // Derive advertised gRPC addr if not provided
    let advertised_grpc = if state.config.public_grpc_addr.is_empty() {
        format!("http://{}", addr)
    } else {
        state.config.public_grpc_addr.clone()
    };
    let gossip_task = tokio::spawn(cluster::run_gossip(swarm, state.cluster.clone(), advertised_grpc));
    let server_task =
        tokio::spawn(async move { axum::serve(listener, app.into_make_service()).await });

    // Run both tasks concurrently.
    let (server_result, gossip_result) = tokio::join!(server_task, gossip_task);
    server_result??;
    gossip_result??;

    Ok(())
}

pub fn create_pool(db_url: &str) -> Result<Pool> {
    let pg_config = tokio_postgres::Config::from_str(db_url)?;
    let mgr_config = ManagerConfig {
        recycling_method: RecyclingMethod::Fast,
    };
    let mgr = deadpool_postgres::Manager::from_config(pg_config, NoTls, mgr_config);
    Pool::builder(mgr).build().map_err(Into::into)
}

pub async fn run_migrations(
    db_url: &str,
    mut runner: refinery::Runner,
    table_name: &str,
) -> Result<()> {
    let (mut client, connection) = tokio_postgres::connect(db_url, NoTls).await?;
    tokio::spawn(async move {
        if let Err(e) = connection.await {
            eprintln!("connection error: {}", e);
        }
    });
    runner
        .set_migration_table_name(table_name)
        .run_async(&mut client)
        .await?;
    Ok(())
}
